{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python 3 notebook for neural network\n",
    "import numpy as np\n",
    "import math\n",
    "import statistics\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "from keras.utils import plot_model\n",
    "import IPython.display\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import importlib\n",
    "import sys\n",
    "import pickle\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "from random import shuffle\n",
    "\n",
    "# https://machinelearningmastery.com/multi-class-classification-tutorial-keras-deep-learning-library/\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.utils import np_utils\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "we = importlib.import_module(\"word_embeddings\")\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' '"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Global Parameters \"\"\"\n",
    "# Load some data\n",
    "wordEmbedDict = we.getWordEmbeddingDict() # Load the dictionary\n",
    "\n",
    "# Labels\n",
    "top_20 = sorted(['AskReddit', 'leagueoflegends', 'nba', 'funny', 'pics', 'nfl', 'pcmasterrace', \\\n",
    "          'videos', 'news', 'todayilearned', 'DestinyTheGame', 'worldnews', 'soccer', \\\n",
    "          'DotA2', 'AdviceAnimals', 'WTF', 'GlobalOffensive', 'hockey', 'movies', 'SquaredCircle'])\n",
    "\n",
    "# Indices of our desired data\n",
    "TRUE_LABEL = 8 # Index of the true label, hard coded\n",
    "BODY_INDEX = 17 # Index of the reddit comment, hard coded\n",
    "\n",
    "# Neural Network Parameters\n",
    "NUM_SUBREDDITS = len(top_20)\n",
    "NUM_FEATURES = 300 # length returned from embedding\n",
    "NUM_EXAMPLES = 15000 # Arbitrary, choose however many we want to grab from the dataset\n",
    "NUM_EPOCHS = 100\n",
    "NUM_HIDDEN_NEURONS = 3\n",
    "NUM_LAYERS = 3\n",
    "LEARNING_RATE = 0.0001\n",
    "\n",
    "SUBREDDIT = \"leagueoflegends\" # used for debugging,delete later\n",
    "unparsed = \"./data/condensed_dataset.pkl\" # will change this so we can just call the whole pkl set\n",
    "\n",
    "# Encoder\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(top_20) # Encodes the NUM_SUBREDDITS subreddits\n",
    "\n",
    "# Seed\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "\"\"\" \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Helper Functions'''\n",
    "\n",
    "# Normalization of dataset\n",
    "\n",
    "# Get mean of a column feature\n",
    "def getMean(column):\n",
    "    sum = 0\n",
    "    n = len(column)\n",
    "    for i in range(n):\n",
    "        sum += column.iloc[i]\n",
    "    mean = sum / float(n)\n",
    "    return mean\n",
    "\n",
    "def getVariance(column, mean):\n",
    "    squareMeanSum = 0\n",
    "    n = len(column)\n",
    "    for i in range(n):\n",
    "        squareMeanSum += (column.iloc[i] - mean)**2\n",
    "    var = math.sqrt(squareMeanSum / float(n))\n",
    "    return var\n",
    "\n",
    "def normalizeSet(set):\n",
    "    numRow = len(set.index)\n",
    "    numCol = len(set.columns)\n",
    "    for col in range(numCol):\n",
    "        column = set.iloc[:,col]\n",
    "        mean = getMean(column)\n",
    "        var = getVariance(column, mean)\n",
    "        \n",
    "        for row in range(numRow):\n",
    "            set.iloc[row, col] = float(set.iloc[row, col] - mean) / var\n",
    "    return set\n",
    "\n",
    "# @param dir: string, directory of pickle data\n",
    "# @return dataset: unpickled dataset\n",
    "def loadPickleData(dir):\n",
    "    with open(dir, 'rb') as f:\n",
    "        dataset = pickle.load(f)\n",
    "    return dataset\n",
    "\n",
    "# Returns [X, Y] with m examples\n",
    "def loadData(pickleDir, m):\n",
    "    pickle = loadPickleData(pickleDir)\n",
    "    return vectorizeDataSet(pickle, m)\n",
    "\n",
    "def stripNonAlpha(word):\n",
    "    word = re.sub(r'\\W+', '', word)\n",
    "    return word\n",
    "\n",
    "def vectorizeWord(word):\n",
    "    word = stripNonAlpha(word)\n",
    "    keyset = wordEmbedDict.keys() # words in the dictionary\n",
    "    zeroVec = np.zeros((1, NUM_FEATURES))\n",
    "    vWord = pd.DataFrame(zeroVec)\n",
    "    \n",
    "    if word in keyset:\n",
    "        vWord = pd.DataFrame(wordEmbedDict[word]).transpose()\n",
    "    return vWord # returns zero vector if the word is not in the dictionary\n",
    "\n",
    "def vectorizeComment(body):\n",
    "#     print(body)\n",
    "    vComment = np.zeros((1, NUM_FEATURES))\n",
    "    vComment = pd.DataFrame(vComment)\n",
    "    words = body.split()\n",
    "#     print(vComment)\n",
    "    \n",
    "    numWords = 0\n",
    "    for word in words:\n",
    "        vWord = vectorizeWord(word)\n",
    "        numWords += 1\n",
    "        vComment = vComment + vWord\n",
    "    vComNP = vComment.values\n",
    "    vComScaled = vComNP * (1/float(numWords))\n",
    "    vComScaled = pd.DataFrame(vComScaled)\n",
    "    return vComScaled\n",
    "\n",
    "# Encodes a subreddit string into an unrolled one-hot pandas vector\n",
    "def oneHotEncode(subreddit):\n",
    "    #   https://machinelearningmastery.com/multi-class-classification-tutorial-keras-deep-learning-library/\n",
    "    encoded_Y = encoder.transform([subreddit])[0] # get the integer category\n",
    "    \n",
    "    oneHot = [0 for _ in range(NUM_SUBREDDITS)]\n",
    "    oneHot[encoded_Y] = 1\n",
    "    pandasOneHot = pd.DataFrame(oneHot)\n",
    "    \n",
    "    return pandasOneHot.transpose()\n",
    "    \n",
    "# Returns a squished array from 3D data \n",
    "# entries are examples, not comments (body)\n",
    "def to2D(data):\n",
    "    data_2D = [example for i in range(NUM_SUBREDDITS) for example in data[i]]\n",
    "    df = pd.DataFrame(data_2D)\n",
    "    return df\n",
    "    \n",
    "    \n",
    "#TODO: check if seed is bad?? just set a new random seed\n",
    "#TODO: limit number of features \n",
    "def vectorizeDataSet(data, m):\n",
    "    data = pd.DataFrame(data)\n",
    "    data = data.sample(frac=1, random_state=88).reset_index(drop=True) # Shuffles data\n",
    "    \n",
    "    data_2D = to2D(data)\n",
    "    \n",
    "    comments = data_2D.pop(BODY_INDEX)\n",
    "#     print(len(comments))\n",
    "    true_labels = data_2D.pop(TRUE_LABEL)\n",
    "    \n",
    "    unrollComment = comments[0]\n",
    "    print(\"unrollcomment: \\n\", unrollComment)\n",
    "    print(type(unrollComment))\n",
    "    X = vectorizeComment(unrollComment)\n",
    "    print(X)\n",
    "    firstSubreddit = true_labels[0]\n",
    "    Y = oneHotEncode(firstSubreddit)\n",
    "\n",
    "    # For each example in old data set, get the actual comment and featurize it into X\n",
    "    # Also get unrolled true label\n",
    "    for i in range(1, m):\n",
    "        comment = comments[i]\n",
    "        example = vectorizeComment(comment)\n",
    "        subreddit = true_labels[i]\n",
    "        oneHot = oneHotEncode(subreddit)\n",
    "        \n",
    "        X = pd.concat([X, example])\n",
    "        Y = pd.concat([Y, oneHot])\n",
    "        \n",
    "    X_scaled = preprocessing.StandardScaler().fit_transform(X)\n",
    "    X_scaled_df = pd.DataFrame(X_scaled)\n",
    "    X_scaled_df.reset_index(drop=True, inplace=True)\n",
    "    Y.reset_index(drop=True, inplace=True)\n",
    "    concat = pd.concat([X_scaled_df, Y], axis=1)\n",
    "    return concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1430438400, 3, 't5_2qh1i', 't3_34f9rh', 't1_cqug90j', 0, None, None, 'AskReddit', 'cqug90j', None, 0, 0, 0, 'jesse9o3', 3, 1432703079, \"No one has a European accent either  because it doesn't exist. There are accents from Europe but not a European accent.\", None, 0, 0, 't1_cqug2sr', 0]\n"
     ]
    }
   ],
   "source": [
    "# Debugging\n",
    "data = loadPickleData(unparsed)\n",
    "print(data[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000\n"
     ]
    }
   ],
   "source": [
    "# Debugging\n",
    "# print(data[2][0])\n",
    "# print(len(data[0]))\n",
    "\n",
    "data_2D = to2D(data)\n",
    "# print(data_2D[0])\n",
    "df = pd.DataFrame(data_2D)\n",
    "# df.tail()\n",
    "comments = df.pop(BODY_INDEX)\n",
    "print(len(comments))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset\n",
    "#https://stackoverflow.com/questions/38250710/how-to-split-data-into-3-sets-train-validation-and-test/38251213#38251213\n",
    "def train_validate_test_split(df, train_percent=.9, validate_percent=.05, seed=seed):\n",
    "    m = len(df.index)\n",
    "    \n",
    "    train_end = int(train_percent * m)\n",
    "    validate_end = int(validate_percent * m) + train_end\n",
    "    train = df.iloc[:train_end]\n",
    "    validate = df.iloc[train_end:validate_end]\n",
    "    test = df.loc[validate_end:]\n",
    "    return train, validate, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unrollcomment: \n",
      " Yeah, but [60% boards,](http://i.imgur.com/gKoR38B.png) and the like, have layouts that are of normal size, just very minimal in all other aspects. What I mean is, all the keys are about the same size, and about the same distance from each other as larger, more regular keyboards; they just have fewer keys and lower profile casings etc.\n",
      "\n",
      "This keyboard looks like all the keys are smaller, and not spaced out to compensate for the size, meaning the whole layout is very compressed. Must be intended for mobile/portable use.\n",
      "<class 'str'>\n",
      "        0         1         2         3         4         5         6    \\\n",
      "0  0.032985  0.008206 -0.114413 -0.188197  0.103022 -0.049788 -3.186192   \n",
      "\n",
      "        7         8         9      ...          290       291      292  \\\n",
      "0  0.259812  0.018643 -0.341993    ...     0.031002 -0.119123 -0.03924   \n",
      "\n",
      "        293       294      295       296      297       298       299  \n",
      "0 -0.011097  0.021538  0.04608 -0.080472 -0.13224  0.058556  0.079446  \n",
      "\n",
      "[1 rows x 300 columns]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "400",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-77-3831f158251e>\u001b[0m in \u001b[0;36mloadData\u001b[0;34m(pickleDir, m)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mloadData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpickleDir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0mpickle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloadPickleData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpickleDir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mvectorizeDataSet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpickle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mstripNonAlpha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-77-3831f158251e>\u001b[0m in \u001b[0;36mvectorizeDataSet\u001b[0;34m(data, m)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;31m# Also get unrolled true label\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m         \u001b[0mcomment\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcomments\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m         \u001b[0mexample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizeComment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomment\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0msubreddit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrue_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    621\u001b[0m         \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 623\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    624\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_value\u001b[0;34m(self, series, key)\u001b[0m\n\u001b[1;32m   2558\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2559\u001b[0m             return self._engine.get_value(s, k,\n\u001b[0;32m-> 2560\u001b[0;31m                                           tz=getattr(series.dtype, 'tz', None))\n\u001b[0m\u001b[1;32m   2561\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2562\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minferred_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'integer'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'boolean'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_value\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_value\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 400"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df = loadData(unparsed, NUM_EXAMPLES)\n",
    "train, validate, test = train_validate_test_split(df)\n",
    "\n",
    "train_labels = train.iloc[:, NUM_FEATURES:]\n",
    "validate_labels = validate.iloc[:, NUM_FEATURES:]\n",
    "test_labels = test.iloc[:, NUM_FEATURES:]\n",
    "\n",
    "train = train.iloc[:, :NUM_FEATURES]\n",
    "validate = validate.iloc[:, :NUM_FEATURES]\n",
    "test = test.iloc[:, :NUM_FEATURES]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 93.8 ms, total: 93.8 ms\n",
      "Wall time: 569 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# # Save the dataframe as a pickle file to be read later\n",
    "# df.to_pickle(\"./data/pandas-pickle-small.pkl\")\n",
    "\n",
    "# check if potentially overwriting data with the same feature everytime\n",
    "unpickled = pd.read_pickle(\"./data/pandas-pickle-small.pkl\")\n",
    "unpickled.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 10.7 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Check softmax output (final layer output)\n",
    "# Check gradients inside the keras model built in function\n",
    "# Neural network function\n",
    "def build_nn(hidden_layer_sizes):\n",
    "    model = keras.Sequential()\n",
    "    \n",
    "    # Input layer\n",
    "    model.add(layers.Dense(hidden_layer_sizes[0], input_dim=NUM_FEATURES, activation='relu'))\n",
    "\n",
    "    # Conv1d Layer to fix high variance\n",
    "#     model.add(layers.Conv1D(kernel_size = (10),strides=10,filters=2, input_shape=(hidden_layer_sizes[0],NUM_FEATURES),kernel_initializer= 'uniform',activation='relu'))\n",
    "    \n",
    "#      https://datascience.stackexchange.com/questions/19407/keras-built-in-multi-layer-shortcut\n",
    "#     Hidden layers\n",
    "    for size in hidden_layer_sizes[1:]:\n",
    "        model.add(layers.Dense(size, activation='relu', kernel_initializer='random_uniform', use_bias=True, activity_regularizer=regulizers.l1(0.01)))\n",
    "        \n",
    "    # Fixes somethings for some reason\n",
    "#     model.add(layers.Flatten())\n",
    "    \n",
    "    # Output layer\n",
    "    model.add(layers.Dense(NUM_SUBREDDITS, activation='softmax'))\n",
    "    \n",
    "    # Optimizer. Can change this to whatever we want\n",
    "    optimizer = tf.keras.optimizers.Adam(lr=LEARNING_RATE)\n",
    "\n",
    "    # Compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy', 'categorical_crossentropy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/esiav/.local/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'regulizers' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36mbuild_nn\u001b[0;34m(hidden_layer_sizes)\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'regulizers' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Build the model\n",
    "# Features might be an issue, pick different subreddits\n",
    "# potentially try out CNN with matrix word embedding without combined word embedding\n",
    "sizes_list = [NUM_HIDDEN_NEURONS for i in range(NUM_LAYERS)]\n",
    "nn_model = build_nn(sizes_list)\n",
    "nn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nn_model' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Get neural network history\n",
    "# Change batch size (inspect what happens with each ITERATION, not EPOCH)\n",
    "# History is the progress of our neural network, will be used to plot cost functions\n",
    "nn_history = nn_model.fit(train, train_labels, epochs=NUM_EPOCHS, verbose=2,\n",
    "         validation_data=(validate, validate_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn_history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-b36d06c20f80>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mplot_history\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn_history\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'nn_history' is not defined"
     ]
    }
   ],
   "source": [
    "# Plot metrics\n",
    "def plot_history(history):\n",
    "    hist = pd.DataFrame(history.history)\n",
    "    hist['epoch'] = history.epoch\n",
    "\n",
    "    plt.figure()\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Cross Entropy Error')\n",
    "    plt.plot(hist['epoch'], hist['categorical_crossentropy'],\n",
    "           label='Train Error')\n",
    "    plt.plot(hist['epoch'], hist['val_categorical_crossentropy'],\n",
    "           label = 'Val Error')\n",
    "    plt.ylim([1,4])\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "plot_history(nn_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See Training Accuracy\n",
    "hist = pd.DataFrame(nn_history.history)\n",
    "accuracy_vec = hist.pop(\"acc\")\n",
    "finalAcc = accuracy_vec[len(accuracy_vec) - 1]\n",
    "print(\"Final accuracy: {}%\".format(finalAcc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expects list\n",
    "def accuracy(predictions, true_label):\n",
    "    count = 0\n",
    "    total = len(predictions)\n",
    "    for i in range(total):\n",
    "        if predictions[i] == true_label[i]:\n",
    "            count += 1\n",
    "    return float(count) / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test accuracy:\n",
    "y_prob = nn_model.predict(test)\n",
    "y_classes = y_prob.argmax(axis=-1)\n",
    "# print(y_classes)\n",
    "# print(test_labels)\n",
    "accuracy(y_classes, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Logistic Regression Model\n",
    "df2 = pd.read_pickle(\"./data/pandas-pickle-small.pkl\")\n",
    "df2 = df2.sample(frac=1, random_state=2).reset_index(drop=True) # Shuffles data\n",
    "# # print(\"This is our X: \", df2.iloc[:, ]\n",
    "numTrain = 9000\n",
    "numTest = 1000\n",
    "\n",
    "X = df2.iloc[0:numTrain, :NUM_FEATURES]\n",
    "X_array = X.values\n",
    "\n",
    "y = df2.iloc[0:numTrain, NUM_FEATURES:]\n",
    "y_array = y.values\n",
    "\n",
    "y_array_rows = y_array.shape[0]\n",
    "\n",
    "y_integer_classes = np.array([])\n",
    "for i in range(y_array_rows):\n",
    "    row = y_array[i]\n",
    "    index = np.where(row==1)[0][0]\n",
    "    y_integer_classes = np.append(y_integer_classes, int(index))\n",
    "\n",
    "testX = df2.iloc[numTrain:numTrain+numTest, :NUM_FEATURES]\n",
    "testX_array = testX.values\n",
    "testY = df2.iloc[numTrain:numTrain+numTest, NUM_FEATURES:]\n",
    "testY_array = testY.values\n",
    "\n",
    "testy_array_rows = testY_array.shape[0]\n",
    "testy_integer_classes = np.array([])\n",
    "for i in range(testy_array_rows):\n",
    "    row = testY_array[i]\n",
    "    index = np.where(row==1)[0][0]\n",
    "    testy_integer_classes = np.append(testy_integer_classes, int(index))\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "n_estimators = 10\n",
    "print(\"Prediction results: \\n\")\n",
    "predictions = OneVsRestClassifier(BaggingClassifier(LinearSVC(random_state=0, max_iter=1000), n_estimators = n_estimators)).fit(X_array, y_integer_classes).predict(testX_array)\n",
    "print(predictions)\n",
    "print(\"True results: \\n\")\n",
    "print(testy_integer_classes)\n",
    "\n",
    "\n",
    "# Expects list\n",
    "def accuracy(predictions, true_label):\n",
    "    count = 0\n",
    "    total = len(predictions)\n",
    "    for i in range(total):\n",
    "        if predictions[i] == true_label[i]:\n",
    "            count += 1\n",
    "    return float(count) / total\n",
    "\n",
    "pred_list = predictions.tolist()\n",
    "testy_list = testy_integer_classes.tolist()\n",
    "print(\"SVM accuracy: {}%\".format(accuracy(pred_list, testy_list)*100))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
