{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python 3 notebook for neural network\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "import statistics\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "from keras.utils import plot_model\n",
    "import IPython.display\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import importlib\n",
    "import sys\n",
    "import pickle\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "from random import shuffle\n",
    "\n",
    "# https://machinelearningmastery.com/multi-class-classification-tutorial-keras-deep-learning-library/\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.utils import np_utils\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "# pnn = importlib.import_module(\"pytorch-neural-network\") # Don't need this anymore, will delete later\n",
    "we = importlib.import_module(\"word_embeddings\")\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' '"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Global Parameters \"\"\"\n",
    "# Load some data\n",
    "wordEmbedDict = we.getWordEmbeddingDict() # Load the dictionary\n",
    "\n",
    "# Labels\n",
    "top_20 = sorted(['AskReddit', 'leagueoflegends', 'nba', 'funny', 'pics', 'nfl', 'pcmasterrace', \\\n",
    "          'videos', 'news', 'todayilearned', 'DestinyTheGame', 'worldnews', 'soccer', \\\n",
    "          'DotA2', 'AdviceAnimals', 'WTF', 'GlobalOffensive', 'hockey', 'movies', 'SquaredCircle'])\n",
    "\n",
    "# Indices of our desired data\n",
    "TRUE_LABEL = 8 # Index of the true label, hard coded\n",
    "BODY_INDEX = 17 # Index of the reddit comment, hard coded\n",
    "\n",
    "# Neural Network Parameters\n",
    "NUM_SUBREDDITS = len(top_20)\n",
    "NUM_FEATURES = 300 # length returned from embedding\n",
    "NUM_EXAMPLES = 500 # Arbitrary, choose however many we want to grab from the dataset\n",
    "NUM_EPOCHS = 20 # plateus really fast... no need to run more epochs\n",
    "NUM_HIDDEN_NEURONS = 100\n",
    "NUM_LAYERS = 20\n",
    "LEARNING_RATE = 0.1\n",
    "\n",
    "SUBREDDIT = \"leagueoflegends\" # used for debugging,delete later\n",
    "unparsed = \"./data/condensed_dataset_SMALL.pkl\" # will change this so we can just call the whole pkl set\n",
    "\n",
    "# Seed\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "\"\"\" \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Helper Functions'''\n",
    "# @param dir: string, directory of pickle data\n",
    "# @return dataset: unpickled dataset\n",
    "def loadPickleData(dir):\n",
    "    with open(dir, 'rb') as f:\n",
    "        dataset = pickle.load(f)\n",
    "    return dataset\n",
    "\n",
    "# Returns [X, Y] with m examples\n",
    "def loadData(pickleDir, m):\n",
    "    pickle = loadPickleData(pickleDir)\n",
    "    return vectorizeDataSet(pickle, m)\n",
    "\n",
    "def stripNonAlpha(word):\n",
    "    word = re.sub(r'\\W+', '', word)\n",
    "    return word\n",
    "\n",
    "def vectorizeWord(word):\n",
    "    word = stripNonAlpha(word)\n",
    "    keyset = wordEmbedDict.keys() # words in the dictionary\n",
    "    zeroVec = np.zeros((1, NUM_FEATURES))\n",
    "    vWord = pd.DataFrame(zeroVec)\n",
    "    \n",
    "    if word in keyset:\n",
    "        vWord = pd.DataFrame(wordEmbedDict[word]).transpose()\n",
    "    return vWord # returns zero vector if the word is not in the dictionary\n",
    "\n",
    "def vectorizeComment(body):\n",
    "    vComment = np.zeros((1, NUM_FEATURES))\n",
    "    vComment = pd.DataFrame(vComment)\n",
    "    words = body.split()\n",
    "    for word in words:\n",
    "        vWord = vectorizeWord(word)\n",
    "        vComment = vComment.add(vWord)\n",
    "        \n",
    "    mean = getMean(vComment)\n",
    "    vComment = [float(value) / mean for value in vComment]\n",
    "    return pd.DataFrame(vComment)\n",
    "\n",
    "# Encodes a subreddit string into an unrolled one-hot pandas vector\n",
    "def oneHotEncode(subreddit):\n",
    "    #   https://machinelearningmastery.com/multi-class-classification-tutorial-keras-deep-learning-library/\n",
    "    encoder = LabelEncoder()\n",
    "    encoder.fit(top_20) # Encodes the NUM_SUBREDDITS subreddits\n",
    "    encoded_Y = encoder.transform([subreddit])[0] # get the integer category\n",
    "    \n",
    "    oneHot = [0 for _ in range(NUM_SUBREDDITS)]\n",
    "    oneHot[encoded_Y] = 1\n",
    "    pandasOneHot = pd.DataFrame(oneHot)\n",
    "    \n",
    "    return pandasOneHot.transpose()\n",
    "    \n",
    "def vectorizeDataSet(data, m):\n",
    "    data = pd.DataFrame(data)\n",
    "    data = data.sample(frac=1, random_state=seed).reset_index(drop=True) # Shuffles data\n",
    "    comments = data.pop(BODY_INDEX)\n",
    "    true_labels = data.pop(TRUE_LABEL)\n",
    "    \n",
    "    unrollComment = comments[0]\n",
    "    X = vectorizeComment(unrollComment)\n",
    "    firstSubreddit = true_labels[0]\n",
    "    Y = oneHotEncode(firstSubreddit)\n",
    "\n",
    "    # For each example in old data set, get the actual comment and featurize it into X\n",
    "    # Also get unrolled true label\n",
    "    for i in range(1, m):\n",
    "        comment = comments[i]\n",
    "        example = vectorizeComment(comment)\n",
    "        subreddit = true_labels[i]\n",
    "        oneHot = oneHotEncode(subreddit)\n",
    "        \n",
    "        X = pd.concat([X, example])\n",
    "        Y = pd.concat([Y, oneHot])\n",
    "    return pd.concat([X, Y], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization of dataset\n",
    "\n",
    "# Get mean of a column feature\n",
    "def getMean(column):\n",
    "    sum = 0\n",
    "    n = len(column)\n",
    "    for i in range(n):\n",
    "        sum += column.iloc[i]\n",
    "    mean = sum / float(n)\n",
    "    return mean\n",
    "\n",
    "def getVariance(column, mean):\n",
    "    squareMeanSum = 0\n",
    "    n = len(column)\n",
    "    for i in range(n):\n",
    "        squareMeanSum += (column.iloc[i] - mean)**2\n",
    "    var = math.sqrt(squareMeanSum / float(n))\n",
    "    return var\n",
    "\n",
    "def normalizeSet(set):\n",
    "    numRow = len(set.index)\n",
    "    numCol = len(set.columns)\n",
    "    for col in range(numCol):\n",
    "        column = set.iloc[:,col]\n",
    "        mean = getMean(column)\n",
    "        var = getVariance(column, mean)\n",
    "        \n",
    "        for row in range(numRow):\n",
    "            set.iloc[row, col] = float(set.iloc[row, col] - mean) / var\n",
    "    return set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset\n",
    "#https://stackoverflow.com/questions/38250710/how-to-split-data-into-3-sets-train-validation-and-test/38251213#38251213\n",
    "def train_validate_test_split(df, train_percent=.9, validate_percent=.05, seed=seed):\n",
    "    m = len(df.index)\n",
    "    \n",
    "    train_end = int(train_percent * m)\n",
    "    validate_end = int(validate_percent * m) + train_end\n",
    "    train = df.iloc[:train_end]\n",
    "    validate = df.iloc[train_end:validate_end]\n",
    "    test = df.loc[validate_end:]\n",
    "    return train, validate, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df = loadData(unparsed, NUM_EXAMPLES)\n",
    "train, validate, test = train_validate_test_split(df)\n",
    "\n",
    "train_labels = train.iloc[:, NUM_FEATURES:]\n",
    "validate_labels = validate.iloc[:, NUM_FEATURES:]\n",
    "test_labels = test.iloc[:, NUM_FEATURES:]\n",
    "\n",
    "train = train.iloc[:, :NUM_FEATURES]\n",
    "validate = validate.iloc[:, :NUM_FEATURES]\n",
    "test = test.iloc[:, :NUM_FEATURES]\n",
    "\n",
    "# normed_train = normalizeSet(train)\n",
    "# normed_validate = normalizeSet(validate)\n",
    "# normed_test = normalizeSet(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 109 ms, total: 109 ms\n",
      "Wall time: 197 ms\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "# # Save the dataframe as a pickle file to be read later\n",
    "# df.to_pickle(\"./data/pandas-pickle-small.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 20.7 Âµs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Neural network function\n",
    "def build_nn(hidden_layer_sizes):\n",
    "    model = keras.Sequential()\n",
    "    \n",
    "    # Input layer\n",
    "    model.add(layers.Dense(hidden_layer_sizes[0], input_dim=NUM_FEATURES, activation='relu'))\n",
    "    \n",
    "#      https://datascience.stackexchange.com/questions/19407/keras-built-in-multi-layer-shortcut\n",
    "#     Hidden layers\n",
    "    for size in hidden_layer_sizes[1:]:\n",
    "        model.add(layers.Dense(size, activation='relu'))\n",
    "    \n",
    "    # Output layer\n",
    "    model.add(layers.Dense(NUM_SUBREDDITS, activation='softmax'))\n",
    "    \n",
    "    # Optimizer. Can change this to whatever we want\n",
    "    optimizer = tf.keras.optimizers.Adam(lr=LEARNING_RATE)\n",
    "\n",
    "    # Compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy', 'categorical_crossentropy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_44 (Dense)             (None, 100)               30100     \n",
      "_________________________________________________________________\n",
      "dense_45 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_46 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_47 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_48 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_49 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_50 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_51 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_52 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_53 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_54 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_55 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_56 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_57 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_58 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_59 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_60 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_61 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_62 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_63 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_64 (Dense)             (None, 20)                2020      \n",
      "=================================================================\n",
      "Total params: 224,020\n",
      "Trainable params: 224,020\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "CPU times: user 516 ms, sys: 0 ns, total: 516 ms\n",
      "Wall time: 518 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Build the model\n",
    "sizes_list = [NUM_HIDDEN_NEURONS for i in range(NUM_LAYERS)]\n",
    "nn_model = build_nn(sizes_list)\n",
    "nn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9000 samples, validate on 500 samples\n",
      "Epoch 1/20\n",
      "9000/9000 [==============================] - 3s 284us/sample - loss: 15.2619 - acc: 0.0503 - categorical_crossentropy: 15.2619 - val_loss: 15.1510 - val_acc: 0.0600 - val_categorical_crossentropy: 15.1510\n",
      "Epoch 2/20\n",
      "9000/9000 [==============================] - 2s 195us/sample - loss: 15.3122 - acc: 0.0500 - categorical_crossentropy: 15.3122 - val_loss: 15.1510 - val_acc: 0.0600 - val_categorical_crossentropy: 15.1510\n",
      "Epoch 3/20\n",
      "9000/9000 [==============================] - 2s 194us/sample - loss: 15.3122 - acc: 0.0500 - categorical_crossentropy: 15.3122 - val_loss: 15.1510 - val_acc: 0.0600 - val_categorical_crossentropy: 15.1510\n",
      "Epoch 4/20\n",
      "9000/9000 [==============================] - 2s 198us/sample - loss: 15.3122 - acc: 0.0500 - categorical_crossentropy: 15.3122 - val_loss: 15.1510 - val_acc: 0.0600 - val_categorical_crossentropy: 15.1510\n",
      "Epoch 5/20\n",
      "9000/9000 [==============================] - 2s 202us/sample - loss: 15.3122 - acc: 0.0500 - categorical_crossentropy: 15.3122 - val_loss: 15.1510 - val_acc: 0.0600 - val_categorical_crossentropy: 15.1510\n",
      "Epoch 6/20\n",
      " 544/9000 [>.............................] - ETA: 1s - loss: 15.4959 - acc: 0.0386 - categorical_crossentropy: 15.4959  "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    878\u001b[0m           \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m           \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m           validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, mode, validation_in_fit, **kwargs)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m         \u001b[0;31m# Get outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 329\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    330\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3074\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3075\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3076\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3077\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3078\u001b[0m     return nest.pack_sequence_as(self._outputs_structure,\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Buggy. Loss goes constant after a while :(\n",
    "# Get neural network history\n",
    "# History is the progress of our neural network, will be used to plot cost functions\n",
    "# nn_history = KerasClassifier(build_fn=nn_model, epochs=NUM_EPOCHS, verbose=2) doesnt work :(\n",
    "nn_history = nn_model.fit(train, train_labels, epochs=NUM_EPOCHS, verbose=1,\n",
    "         validation_data=(validate, validate_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f5d3bac2e48>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot metrics\n",
    "def plot_history(history):\n",
    "    hist = pd.DataFrame(history.history)\n",
    "    hist['epoch'] = history.epoch\n",
    "\n",
    "    plt.figure()\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Cross Entropy Error')\n",
    "    plt.plot(hist['epoch'], hist['categorical_crossentropy'],\n",
    "           label='Train Error')\n",
    "    plt.plot(hist['epoch'], hist['val_categorical_crossentropy'],\n",
    "           label = 'Val Error')\n",
    "    plt.ylim([1,20])\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "plot_history(nn_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final accuracy: 4.911111295223236%\n"
     ]
    }
   ],
   "source": [
    "# See accuracy\n",
    "hist = pd.DataFrame(nn_history.history)\n",
    "accuracy_vec = hist.pop(\"acc\")\n",
    "finalAcc = accuracy_vec[len(accuracy_vec) - 1]\n",
    "print(\"Final accuracy: {}%\".format(finalAcc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           0          1          2          3          4          5   \\\n",
      "0   -5.987771   2.936097  -5.050001  -5.734839   1.612856   1.673172   \n",
      "0   -0.494922  -1.965185  -2.644808  -3.261982  -0.655925  -0.323322   \n",
      "0   -7.849946   6.617751  -5.658960  -8.995060   5.718487   3.404609   \n",
      "0   -1.765055   2.141997  -1.865215  -1.761664   0.748893   0.046464   \n",
      "0   -0.212461   2.575485  -1.058007   0.756325   0.464451   1.366716   \n",
      "0   -2.237989   1.564746  -2.971133  -2.339031   0.408509   1.214885   \n",
      "0   -2.648789  21.753350 -10.909448 -30.522799  26.116216 -12.309386   \n",
      "0   -6.840692  -1.368033 -11.509770 -17.310337   7.490556  -9.815082   \n",
      "0   -0.149701   0.433019  -2.125095  -3.990538   4.219438   1.674350   \n",
      "0   -3.007357   4.682000  -4.101943 -10.033141   5.855818  -1.603001   \n",
      "0    0.445483   0.802003  -4.980627  -8.168943   4.890778  -2.903288   \n",
      "0    0.710743  -1.565472  -3.424123  -6.578267   2.052023   1.949281   \n",
      "0   -1.094742  -0.776882  -3.889573  -0.139700   1.270287   0.428621   \n",
      "0   -3.329411  -3.632432  -2.357810 -14.666201   7.355468  -4.017106   \n",
      "0   -4.833464   4.173186  -3.262184 -12.939687  11.341346   4.903327   \n",
      "0   -0.772619   0.149158  -3.221134  -7.386694   2.962900   3.214572   \n",
      "0   -2.201952   2.349621  -1.393807  -1.250164   2.235758   2.198740   \n",
      "0   -7.693642  10.864729  -6.932947 -14.582477   6.165103  -9.360547   \n",
      "0   -0.803517   1.353832  -4.256873  -5.079040   3.907224  -2.826752   \n",
      "0   -1.980984  -0.662479  -1.157342  -1.661226   2.203029  -1.990812   \n",
      "0   -0.248981   0.242962  -0.424340  -2.129798   0.785865  -2.142299   \n",
      "0    1.748373  -1.532441  -1.074629 -10.488076   3.137466  -5.348352   \n",
      "0   -1.327660  -0.251792  -1.165782  -3.755873  -0.234552   0.991781   \n",
      "0   -5.665061   3.190416 -11.694329 -14.856098   0.690028   1.988642   \n",
      "0    0.038263   1.196218  -2.715310  -7.630871   3.861207   0.412195   \n",
      "0   -4.556747   0.842080  -1.542787  -5.274713  -6.032834   2.797419   \n",
      "0   -3.420022   0.393171   3.311846 -11.725262  -0.195345  -2.999724   \n",
      "0   -0.829074  -3.021116  -2.873041  -4.792841   3.810738   0.697257   \n",
      "0   -1.963829   0.665964  -0.520792  -5.466274   0.966978  -0.468616   \n",
      "0   -0.617151  -1.807515  -0.715155  -4.646771   3.114371  -2.935947   \n",
      "..        ...        ...        ...        ...        ...        ...   \n",
      "0   -2.379773   0.319536  -2.393338  -5.991982   0.046219   2.496874   \n",
      "0   -1.921621  -2.812286  -1.792003  -3.981945   0.407251  -0.751079   \n",
      "0   -1.192623  -1.094232  -1.125057  -1.933295   0.231761   0.031340   \n",
      "0  -10.103718   4.066444  -0.920727 -12.273718   2.405312  -7.409007   \n",
      "0   -2.151833   2.346452   0.490378  -2.590195   1.568054  -1.504232   \n",
      "0  -21.896701  11.536776 -13.246632 -24.953436  10.876142   2.182546   \n",
      "0   -5.391550  -1.944283  -4.915751  -3.586086   2.076254  -0.709036   \n",
      "0   -2.576226   4.525670  -4.056161  -4.183259   5.536770  -1.678626   \n",
      "0   -3.592525  14.653705 -10.548678 -22.162934  10.358769  -1.971715   \n",
      "0   -1.426333   0.018121  -2.682427  -1.405605   0.110448   0.459651   \n",
      "0   -1.684971  -1.004345  -0.400459  -5.974075   1.465320  -1.722718   \n",
      "0   -2.612699   2.965799  -0.180116  -5.951557   2.608804   0.109924   \n",
      "0   -5.022510  -1.720518  -7.410275 -10.602072   1.705965   0.937499   \n",
      "0   -1.005750  -0.542454  -1.287629  -1.683016   0.905317   1.667745   \n",
      "0   -1.233332  -0.825699  -6.423749 -13.830846  -1.798887  -1.072423   \n",
      "0   -4.578775   1.588135  -0.614291  -4.488649   0.872978  -0.461124   \n",
      "0   -1.979556   4.550772  -4.672219  -8.993417   3.669650  -2.850837   \n",
      "0   -3.134321   7.176315   1.734832 -12.839010   4.303726 -11.240005   \n",
      "0   -4.867474   1.343293  -3.485467  -8.587650   4.464438  -1.143624   \n",
      "0    0.356369   3.532854  -0.331031  -1.378890   1.459763   1.565241   \n",
      "0   -1.138228  -1.040082  -2.309740  -3.731737   2.583179   0.812100   \n",
      "0   -0.049057   1.439117  -4.645375  -3.140131   1.317846   0.421138   \n",
      "0   -0.893040   1.653796  -4.514848  -6.672478   3.365691  -3.843735   \n",
      "0   -3.158678   3.139730  -3.885752  -3.887489   1.425700   0.901256   \n",
      "0   -1.466383   1.159219  -0.075042  -3.875073   0.363460  -0.544247   \n",
      "0    0.367342   1.308372   0.201091  -3.792920   1.512685  -1.171437   \n",
      "0   -2.128425   2.949559  -9.924261 -14.220757   2.549121  -0.092738   \n",
      "0   -4.771319  -1.761555  -3.291630 -19.368265   6.209984  -4.717876   \n",
      "0   -4.386540   0.150851  -1.026398  -5.580354   2.035692  -4.830088   \n",
      "0   -2.028093   4.679899  -2.142423  -3.440545  -1.290550   0.699776   \n",
      "\n",
      "           6          7         8           9  ...  10  11  12  13  14  15  \\\n",
      "0  -184.55650  13.358369 -0.944601  -17.480203 ...   0   0   0   0   0   0   \n",
      "0   -73.18360   8.529020 -0.441500   -8.109405 ...   0   0   0   0   0   0   \n",
      "0  -258.70120  18.117370 -2.688486  -28.026530 ...   0   0   0   0   0   0   \n",
      "0   -42.76970   3.648269 -1.245894   -4.647408 ...   0   0   0   1   0   0   \n",
      "0   -18.29454   3.311180  0.096405   -2.417962 ...   0   0   0   0   0   1   \n",
      "0   -87.18410   4.780240  3.629565  -11.243608 ...   0   0   0   0   0   0   \n",
      "0  -850.17670  65.313732  6.818853 -100.967219 ...   0   0   0   0   0   0   \n",
      "0  -334.24701  17.051056 -0.838245  -32.565217 ...   0   0   0   0   0   1   \n",
      "0   -74.24930   4.219440 -1.572122   -8.520187 ...   0   0   0   0   1   0   \n",
      "0  -260.04060  17.146515  1.776276  -32.243521 ...   0   0   0   1   0   0   \n",
      "0  -224.71420  12.832306  1.439704  -26.471110 ...   0   0   0   0   0   0   \n",
      "0  -131.78160   4.320849  0.848289  -17.047391 ...   1   0   0   0   0   0   \n",
      "0  -122.38120   9.173087 -2.378676  -17.613387 ...   0   0   0   0   0   0   \n",
      "0  -224.23020  12.767019 -1.187749  -24.974272 ...   0   0   0   0   0   0   \n",
      "0  -314.95080  14.894668 -4.161223  -42.391778 ...   0   0   0   0   1   0   \n",
      "0  -185.70580  15.155143 -3.931557  -22.722578 ...   0   0   0   0   0   0   \n",
      "0   -89.89210   5.335167 -1.157929   -9.028276 ...   0   0   0   0   0   0   \n",
      "0  -388.39880  17.041463  0.458672  -50.830098 ...   0   0   0   0   0   0   \n",
      "0  -106.99230   5.299680 -2.326283  -15.382018 ...   0   0   0   0   1   0   \n",
      "0   -92.38010   2.032548  0.535696  -13.862614 ...   0   0   0   0   0   0   \n",
      "0   -45.52390   4.067776 -0.470516   -3.998731 ...   0   0   0   0   0   0   \n",
      "0  -226.54990  11.367999  0.747129  -23.983128 ...   0   0   0   0   0   0   \n",
      "0  -125.16710   3.666084  2.967014  -16.487521 ...   0   0   1   0   0   0   \n",
      "0  -385.70730  13.853415 -1.081521  -36.216995 ...   0   0   0   0   0   0   \n",
      "0  -217.51460  21.300149  2.078906  -25.541264 ...   0   0   0   0   0   1   \n",
      "0  -179.08000  13.467771  2.786763  -19.106354 ...   0   0   0   0   0   0   \n",
      "0  -259.74090  13.621725  0.036132  -25.562544 ...   0   0   0   0   0   0   \n",
      "0  -123.11303  15.616270 -0.372158  -10.356650 ...   0   0   0   0   0   1   \n",
      "0   -52.04910   2.485178  0.728175   -4.733687 ...   0   0   0   0   0   0   \n",
      "0  -107.83900   9.418387 -0.650987  -10.299498 ...   0   0   0   0   0   0   \n",
      "..        ...        ...       ...         ... ...  ..  ..  ..  ..  ..  ..   \n",
      "0  -124.22220   4.938562 -2.391049  -14.975432 ...   1   0   0   0   0   0   \n",
      "0  -117.05520   9.593830 -0.554589  -14.427804 ...   0   0   0   0   0   0   \n",
      "0   -43.85820   1.947630  1.196531   -5.956149 ...   1   0   0   0   0   0   \n",
      "0  -333.72140  25.883149  3.353434  -41.062868 ...   0   0   0   0   0   0   \n",
      "0   -90.85240   7.818096  0.631511   -8.618200 ...   0   0   0   0   0   0   \n",
      "0  -758.47100  57.440484  5.865849  -84.126741 ...   0   0   0   0   0   0   \n",
      "0  -163.35640  12.364200  5.020098  -20.908234 ...   0   0   0   0   0   0   \n",
      "0  -136.02030   7.005713 -0.389775  -18.777033 ...   0   0   0   1   0   0   \n",
      "0  -554.45335  47.585295  6.300376  -62.553903 ...   0   1   0   0   0   0   \n",
      "0   -39.68230   2.523725  0.518188   -5.518272 ...   0   0   0   0   0   0   \n",
      "0  -152.42760  10.230447  1.094269  -15.022723 ...   0   0   0   0   0   0   \n",
      "0  -135.52430   7.428888 -0.582730  -13.829359 ...   0   0   0   0   0   0   \n",
      "0  -219.85700   9.540999  1.630436  -23.038150 ...   0   0   0   0   0   0   \n",
      "0   -52.36230   0.882566 -0.656577   -6.260104 ...   0   0   0   0   0   0   \n",
      "0  -298.89320   9.749027  0.952765  -25.533560 ...   0   0   0   0   0   0   \n",
      "0  -101.97980   4.724742 -1.193888  -14.256967 ...   0   0   0   0   0   0   \n",
      "0  -269.81180  18.906336 -0.866198  -34.534696 ...   0   0   0   0   0   0   \n",
      "0  -383.55960  25.503062  1.867034  -46.685412 ...   0   1   0   0   0   0   \n",
      "0  -164.65180  12.682798 -2.078071  -20.282777 ...   0   1   0   0   0   0   \n",
      "0   -85.10060   0.235373 -0.601316   -8.482543 ...   0   0   0   0   0   0   \n",
      "0   -93.95690   7.330952  2.237719  -12.817293 ...   0   0   0   0   1   0   \n",
      "0  -107.45330   6.253589  0.743144  -11.033954 ...   0   0   1   0   0   0   \n",
      "0  -140.13330  10.061654  2.002541  -20.550095 ...   0   0   0   1   0   0   \n",
      "0  -102.83770   8.166902  1.002524  -12.618830 ...   0   0   0   0   0   0   \n",
      "0   -81.23530   6.821559  0.367922  -10.201483 ...   0   1   0   0   0   0   \n",
      "0   -69.51420   4.771135  0.642789   -8.226592 ...   0   0   0   0   0   0   \n",
      "0  -258.15574  21.234065  1.738370  -24.388043 ...   0   0   1   0   0   0   \n",
      "0  -413.18892  38.096338  5.955578  -32.400210 ...   0   0   0   0   0   0   \n",
      "0   -77.26690   4.216888  0.398812  -10.544630 ...   0   0   0   0   0   0   \n",
      "0  -117.00810   7.079300  1.763391  -11.121207 ...   1   0   0   0   0   0   \n",
      "\n",
      "    16  17  18  19  \n",
      "0    0   0   0   0  \n",
      "0    0   0   1   0  \n",
      "0    0   0   0   0  \n",
      "0    0   0   0   0  \n",
      "0    0   0   0   0  \n",
      "0    0   0   0   0  \n",
      "0    0   0   0   0  \n",
      "0    0   0   0   0  \n",
      "0    0   0   0   0  \n",
      "0    0   0   0   0  \n",
      "0    0   1   0   0  \n",
      "0    0   0   0   0  \n",
      "0    0   0   0   0  \n",
      "0    0   0   0   0  \n",
      "0    0   0   0   0  \n",
      "0    0   0   0   0  \n",
      "0    0   0   1   0  \n",
      "0    1   0   0   0  \n",
      "0    0   0   0   0  \n",
      "0    0   0   0   0  \n",
      "0    0   0   0   0  \n",
      "0    0   0   0   0  \n",
      "0    0   0   0   0  \n",
      "0    0   0   0   0  \n",
      "0    0   0   0   0  \n",
      "0    0   0   0   0  \n",
      "0    0   0   0   0  \n",
      "0    0   0   0   0  \n",
      "0    0   0   0   0  \n",
      "0    0   1   0   0  \n",
      "..  ..  ..  ..  ..  \n",
      "0    0   0   0   0  \n",
      "0    0   0   0   0  \n",
      "0    0   0   0   0  \n",
      "0    0   0   0   0  \n",
      "0    0   0   0   1  \n",
      "0    0   0   0   0  \n",
      "0    0   1   0   0  \n",
      "0    0   0   0   0  \n",
      "0    0   0   0   0  \n",
      "0    0   0   1   0  \n",
      "0    0   0   0   0  \n",
      "0    0   0   1   0  \n",
      "0    0   0   0   0  \n",
      "0    0   0   0   0  \n",
      "0    0   0   0   0  \n",
      "0    0   0   0   0  \n",
      "0    1   0   0   0  \n",
      "0    0   0   0   0  \n",
      "0    0   0   0   0  \n",
      "0    1   0   0   0  \n",
      "0    0   0   0   0  \n",
      "0    0   0   0   0  \n",
      "0    0   0   0   0  \n",
      "0    0   0   0   0  \n",
      "0    0   0   0   0  \n",
      "0    0   0   0   0  \n",
      "0    0   0   0   0  \n",
      "0    0   0   0   1  \n",
      "0    0   0   0   0  \n",
      "0    0   0   0   0  \n",
      "\n",
      "[10000 rows x 320 columns]\n",
      "(9000, 300)\n",
      "(9000, 20)\n",
      "CPU times: user 62.5 ms, sys: 31.2 ms, total: 93.8 ms\n",
      "Wall time: 79.4 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Logistic Regression Model\n",
    "df = pd.read_pickle(\"./data/pandas-pickle-small.pkl\")\n",
    "print(df)\n",
    "X = df.iloc[0:3, ]\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "# print(type(X))\n",
    "# X = df.iloc[:, :NUM_FEATURES]\n",
    "\n",
    "# y = df.iloc[:]\n",
    "# clf = LogisticRegression(random_state=0, solver='lbfgs',\\\n",
    "#                           multi_class='multinomial').fit(X, y)\n",
    "\n",
    "# clf.predict(test)\n",
    "# clf.predict_proba(test)\n",
    "# clf.score(X, y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
