{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python 3 notebook for neural network\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "import statistics\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "from keras.utils import plot_model\n",
    "import IPython.display\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import importlib\n",
    "import sys\n",
    "import pickle\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "from random import shuffle\n",
    "\n",
    "# https://machinelearningmastery.com/multi-class-classification-tutorial-keras-deep-learning-library/\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.utils import np_utils\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "# pnn = importlib.import_module(\"pytorch-neural-network\") # Don't need this anymore, will delete later\n",
    "we = importlib.import_module(\"word_embeddings\")\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' '"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Global Parameters \"\"\"\n",
    "# Load some data\n",
    "wordEmbedDict = we.getWordEmbeddingDict() # Load the dictionary\n",
    "\n",
    "# Labels\n",
    "top_20 = sorted(['AskReddit', 'leagueoflegends', 'nba', 'funny', 'pics', 'nfl', 'pcmasterrace', \\\n",
    "          'videos', 'news', 'todayilearned', 'DestinyTheGame', 'worldnews', 'soccer', \\\n",
    "          'DotA2', 'AdviceAnimals', 'WTF', 'GlobalOffensive', 'hockey', 'movies', 'SquaredCircle'])\n",
    "\n",
    "# Indices of our desired data\n",
    "TRUE_LABEL = 8 # Index of the true label, hard coded\n",
    "BODY_INDEX = 17 # Index of the reddit comment, hard coded\n",
    "\n",
    "# Neural Network Parameters\n",
    "NUM_SUBREDDITS = len(top_20)\n",
    "NUM_FEATURES = 300 # length returned from embedding\n",
    "NUM_EXAMPLES = 10000 # Arbitrary, choose however many we want to grab from the dataset\n",
    "NUM_EPOCHS = 20 # plateus really fast... no need to run more epochs\n",
    "NUM_HIDDEN_NEURONS = 100\n",
    "NUM_LAYERS = 20\n",
    "LEARNING_RATE = 0.1\n",
    "\n",
    "SUBREDDIT = \"leagueoflegends\" # used for debugging,delete later\n",
    "unparsed = \"./data/condensed_dataset_SMALL.pkl\" # will change this so we can just call the whole pkl set\n",
    "\n",
    "# Seed\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "\"\"\" \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Helper Functions'''\n",
    "# @param dir: string, directory of pickle data\n",
    "# @return dataset: unpickled dataset\n",
    "def loadPickleData(dir):\n",
    "    with open(dir, 'rb') as f:\n",
    "        dataset = pickle.load(f)\n",
    "    return dataset\n",
    "\n",
    "# Returns [X, Y] with m examples\n",
    "def loadData(pickleDir, m):\n",
    "    pickle = loadPickleData(pickleDir)\n",
    "    return vectorizeDataSet(pickle, m)\n",
    "\n",
    "def stripNonAlpha(word):\n",
    "    word = re.sub(r'\\W+', '', word)\n",
    "    return word\n",
    "\n",
    "def vectorizeWord(word):\n",
    "    word = stripNonAlpha(word)\n",
    "    keyset = wordEmbedDict.keys() # words in the dictionary\n",
    "    zeroVec = np.zeros((1, NUM_FEATURES))\n",
    "    vWord = pd.DataFrame(zeroVec)\n",
    "    \n",
    "    if word in keyset:\n",
    "        vWord = pd.DataFrame(wordEmbedDict[word]).transpose()\n",
    "    return vWord # returns zero vector if the word is not in the dictionary\n",
    "\n",
    "def vectorizeComment(body):\n",
    "    vComment = np.zeros((1, NUM_FEATURES))\n",
    "    vComment = pd.DataFrame(vComment)\n",
    "    words = body.split()\n",
    "    for word in words:\n",
    "        vWord = vectorizeWord(word)\n",
    "        vComment = vComment.add(vWord)\n",
    "        \n",
    "    mean = getMean(vComment)\n",
    "    vComment = [float(value) / mean for value in vComment]\n",
    "    return vComment\n",
    "\n",
    "# Encodes a subreddit string into an unrolled one-hot pandas vector\n",
    "def oneHotEncode(subreddit):\n",
    "    #   https://machinelearningmastery.com/multi-class-classification-tutorial-keras-deep-learning-library/\n",
    "    encoder = LabelEncoder()\n",
    "    encoder.fit(top_20) # Encodes the NUM_SUBREDDITS subreddits\n",
    "    encoded_Y = encoder.transform([subreddit])[0] # get the integer category\n",
    "    \n",
    "    oneHot = [0 for _ in range(NUM_SUBREDDITS)]\n",
    "    oneHot[encoded_Y] = 1\n",
    "    pandasOneHot = pd.DataFrame(oneHot)\n",
    "    \n",
    "    return pandasOneHot.transpose()\n",
    "    \n",
    "def vectorizeDataSet(data, m):\n",
    "    data = pd.DataFrame(data)\n",
    "    data = data.sample(frac=1, random_state=seed).reset_index(drop=True) # Shuffles data\n",
    "    comments = data.pop(BODY_INDEX)\n",
    "    true_labels = data.pop(TRUE_LABEL)\n",
    "    \n",
    "    unrollComment = comments[0]\n",
    "    X = vectorizeComment(unrollComment)\n",
    "    firstSubreddit = true_labels[0]\n",
    "    Y = oneHotEncode(firstSubreddit)\n",
    "\n",
    "    # For each example in old data set, get the actual comment and featurize it into X\n",
    "    # Also get unrolled true label\n",
    "    for i in range(1, m):\n",
    "        comment = comments[i]\n",
    "        example = vectorizeComment(comment)\n",
    "        subreddit = true_labels[i]\n",
    "        oneHot = oneHotEncode(subreddit)\n",
    "        \n",
    "        X = pd.concat([X, example])\n",
    "        Y = pd.concat([Y, oneHot])\n",
    "    return pd.concat([X, Y], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization of dataset\n",
    "\n",
    "# Get mean of a column feature\n",
    "def getMean(column):\n",
    "    sum = 0\n",
    "    n = len(column)\n",
    "    for i in range(n):\n",
    "        sum += column.iloc[i]\n",
    "    mean = sum / float(n)\n",
    "    return mean\n",
    "\n",
    "def getVariance(column, mean):\n",
    "    squareMeanSum = 0\n",
    "    n = len(column)\n",
    "    for i in range(n):\n",
    "        squareMeanSum += (column.iloc[i] - mean)**2\n",
    "    var = math.sqrt(squareMeanSum / float(n))\n",
    "    return var\n",
    "\n",
    "def normalizeSet(set):\n",
    "    numRow = len(set.index)\n",
    "    numCol = len(set.columns)\n",
    "    for col in range(numCol):\n",
    "        column = set.iloc[:,col]\n",
    "        mean = getMean(column)\n",
    "        var = getVariance(column, mean)\n",
    "        \n",
    "        for row in range(numRow):\n",
    "            set.iloc[row, col] = float(set.iloc[row, col] - mean) / var\n",
    "    return set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset\n",
    "#https://stackoverflow.com/questions/38250710/how-to-split-data-into-3-sets-train-validation-and-test/38251213#38251213\n",
    "def train_validate_test_split(df, train_percent=.9, validate_percent=.05, seed=seed):\n",
    "    m = len(df.index)\n",
    "    \n",
    "    train_end = int(train_percent * m)\n",
    "    validate_end = int(validate_percent * m) + train_end\n",
    "    train = df.iloc[:train_end]\n",
    "    validate = df.iloc[train_end:validate_end]\n",
    "    test = df.loc[validate_end:]\n",
    "    return train, validate, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7min 25s, sys: 2min 44s, total: 10min 10s\n",
      "Wall time: 10min 10s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df = loadData(unparsed, NUM_EXAMPLES)\n",
    "train, validate, test = train_validate_test_split(df)\n",
    "\n",
    "train_labels = train.iloc[:, NUM_FEATURES:]\n",
    "validate_labels = validate.iloc[:, NUM_FEATURES:]\n",
    "test_labels = test.iloc[:, NUM_FEATURES:]\n",
    "\n",
    "train = train.iloc[:, :NUM_FEATURES]\n",
    "validate = validate.iloc[:, :NUM_FEATURES]\n",
    "test = test.iloc[:, :NUM_FEATURES]\n",
    "\n",
    "# normed_train = normalizeSet(train)\n",
    "# normed_validate = normalizeSet(validate)\n",
    "# normed_test = normalizeSet(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 109 ms, total: 109 ms\n",
      "Wall time: 197 ms\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "# # Save the dataframe as a pickle file to be read later\n",
    "# df.to_pickle(\"./data/pandas-pickle-small.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 20.7 Âµs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Neural network function\n",
    "def build_nn(hidden_layer_sizes):\n",
    "    model = keras.Sequential()\n",
    "    \n",
    "    # Input layer\n",
    "    model.add(layers.Dense(hidden_layer_sizes[0], input_dim=NUM_FEATURES, activation='relu'))\n",
    "    \n",
    "#      https://datascience.stackexchange.com/questions/19407/keras-built-in-multi-layer-shortcut\n",
    "#     Hidden layers\n",
    "    for size in hidden_layer_sizes[1:]:\n",
    "        model.add(layers.Dense(size, activation='relu'))\n",
    "    \n",
    "    # Output layer\n",
    "    model.add(layers.Dense(NUM_SUBREDDITS, activation='softmax'))\n",
    "    \n",
    "    # Optimizer. Can change this to whatever we want\n",
    "    optimizer = tf.keras.optimizers.Adam(lr=LEARNING_RATE)\n",
    "\n",
    "    # Compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy', 'categorical_crossentropy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_44 (Dense)             (None, 100)               30100     \n",
      "_________________________________________________________________\n",
      "dense_45 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_46 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_47 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_48 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_49 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_50 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_51 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_52 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_53 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_54 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_55 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_56 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_57 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_58 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_59 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_60 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_61 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_62 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_63 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_64 (Dense)             (None, 20)                2020      \n",
      "=================================================================\n",
      "Total params: 224,020\n",
      "Trainable params: 224,020\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "CPU times: user 516 ms, sys: 0 ns, total: 516 ms\n",
      "Wall time: 518 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Build the model\n",
    "sizes_list = [NUM_HIDDEN_NEURONS for i in range(NUM_LAYERS)]\n",
    "nn_model = build_nn(sizes_list)\n",
    "nn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9000 samples, validate on 500 samples\n",
      "Epoch 1/20\n",
      "9000/9000 [==============================] - 3s 284us/sample - loss: 15.2619 - acc: 0.0503 - categorical_crossentropy: 15.2619 - val_loss: 15.1510 - val_acc: 0.0600 - val_categorical_crossentropy: 15.1510\n",
      "Epoch 2/20\n",
      "9000/9000 [==============================] - 2s 195us/sample - loss: 15.3122 - acc: 0.0500 - categorical_crossentropy: 15.3122 - val_loss: 15.1510 - val_acc: 0.0600 - val_categorical_crossentropy: 15.1510\n",
      "Epoch 3/20\n",
      "9000/9000 [==============================] - 2s 194us/sample - loss: 15.3122 - acc: 0.0500 - categorical_crossentropy: 15.3122 - val_loss: 15.1510 - val_acc: 0.0600 - val_categorical_crossentropy: 15.1510\n",
      "Epoch 4/20\n",
      "9000/9000 [==============================] - 2s 198us/sample - loss: 15.3122 - acc: 0.0500 - categorical_crossentropy: 15.3122 - val_loss: 15.1510 - val_acc: 0.0600 - val_categorical_crossentropy: 15.1510\n",
      "Epoch 5/20\n",
      "9000/9000 [==============================] - 2s 202us/sample - loss: 15.3122 - acc: 0.0500 - categorical_crossentropy: 15.3122 - val_loss: 15.1510 - val_acc: 0.0600 - val_categorical_crossentropy: 15.1510\n",
      "Epoch 6/20\n",
      " 544/9000 [>.............................] - ETA: 1s - loss: 15.4959 - acc: 0.0386 - categorical_crossentropy: 15.4959  "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    878\u001b[0m           \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m           \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m           validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, mode, validation_in_fit, **kwargs)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m         \u001b[0;31m# Get outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 329\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    330\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3074\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3075\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3076\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3077\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3078\u001b[0m     return nest.pack_sequence_as(self._outputs_structure,\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Buggy. Loss goes constant after a while :(\n",
    "# Get neural network history\n",
    "# History is the progress of our neural network, will be used to plot cost functions\n",
    "# nn_history = KerasClassifier(build_fn=nn_model, epochs=NUM_EPOCHS, verbose=2) doesnt work :(\n",
    "nn_history = nn_model.fit(train, train_labels, epochs=NUM_EPOCHS, verbose=1,\n",
    "         validation_data=(validate, validate_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f5d3bac2e48>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot metrics\n",
    "def plot_history(history):\n",
    "    hist = pd.DataFrame(history.history)\n",
    "    hist['epoch'] = history.epoch\n",
    "\n",
    "    plt.figure()\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Cross Entropy Error')\n",
    "    plt.plot(hist['epoch'], hist['categorical_crossentropy'],\n",
    "           label='Train Error')\n",
    "    plt.plot(hist['epoch'], hist['val_categorical_crossentropy'],\n",
    "           label = 'Val Error')\n",
    "    plt.ylim([1,20])\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "plot_history(nn_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final accuracy: 4.911111295223236%\n"
     ]
    }
   ],
   "source": [
    "# See accuracy\n",
    "hist = pd.DataFrame(nn_history.history)\n",
    "accuracy_vec = hist.pop(\"acc\")\n",
    "finalAcc = accuracy_vec[len(accuracy_vec) - 1]\n",
    "print(\"Final accuracy: {}%\".format(finalAcc*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
